{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sagemaker Mxnet/gluon Example\n",
    "Let's use the final project dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker.mxnet import MXNet\n",
    "from mxnet import gluon\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading the dataset from our s3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     ID                                               data  label\n",
      "0  1588  [[[255, 255, 255], [255, 255, 255], [255, 255,...      4\n",
      "1  2011  [[[255, 255, 255], [255, 255, 255], [255, 255,...      1\n",
      "2  1321  [[[255, 255, 255], [255, 255, 255], [255, 255,...      4\n",
      "3   669  [[[255, 255, 255], [255, 255, 255], [255, 255,...      3\n",
      "4  1674  [[[255, 255, 255], [255, 255, 255], [255, 255,...      1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "df = pd.read_pickle(\"./training_data.pkl\")\n",
    "tf = pd.read_pickle(\"./test_data.pkl\")\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparing the data for xnet/gluon format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages/skimage/transform/_warps.py:84: UserWarning: The default mode, 'constant', will be changed to 'reflect' in skimage 0.15.\n",
      "  warn(\"The default mode, 'constant', will be changed to 'reflect' in \"\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from skimage.transform import resize\n",
    "\n",
    "image_vals = df[\"data\"].values\n",
    "label_vals = df[\"label\"].values\n",
    "\n",
    "del df\n",
    "\n",
    "#use tf\n",
    "del tf\n",
    "\n",
    "# Create image and label arrays\n",
    "images = np.zeros((image_vals.shape[0], 3, 224, 224))\n",
    "labels = np.zeros((label_vals.shape[0]))\n",
    "\n",
    "# Iterate through the dataframe row by row\n",
    "for i, (im, label) in enumerate(zip(image_vals, label_vals)):\n",
    "    # Get image from the data column of the current row\n",
    "    \n",
    "    # We need a fixed size input, our images have different sizes, let's pick 224x224.\n",
    "    # Resize image below\n",
    "    im = resize(im, output_shape=(224, 224))\n",
    "    \n",
    "    # Gluon/mxnet expects images in this format (channel, row, column)\n",
    "    # This is the opposite of (row, column, channel), let's fix it\n",
    "    im = np.moveaxis(im, -1, 0)\n",
    "    \n",
    "    # Assign the value in the image array\n",
    "    images[i] = im\n",
    "    \n",
    "    # Assign the label in the label array\n",
    "    labels[i] = label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting the data into Training, Validation, Test and saving into s3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(images, labels, stratify=labels, test_size=0.20)\n",
    "X_test, X_val, y_test, y_val = train_test_split(X_test, y_test, stratify=y_test, test_size=0.5)\n",
    "\n",
    "# Save training and validation data locally\n",
    "np.save('train_data', X_train)\n",
    "np.save('train_label', y_train)\n",
    "np.save('validation_data', X_val)\n",
    "np.save('validation_label', y_val)\n",
    "np.save('test_data', X_test)\n",
    "np.save('test_label', y_test)\n",
    "\n",
    "prefix = 'mla-cv-sagemaker-demo'\n",
    "bucket = 'gmenende-mla-training'\n",
    "\n",
    "boto3.Session().resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'training/train_data.npy')).upload_file('train_data.npy')\n",
    "boto3.Session().resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'training/train_label.npy')).upload_file('train_label.npy')\n",
    "boto3.Session().resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'validation/validation_data.npy')).upload_file('validation_data.npy')\n",
    "boto3.Session().resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'validation/validation_label.npy')).upload_file('validation_label.npy')\n",
    "boto3.Session().resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'test/test_data.npy')).upload_file('test_data.npy')\n",
    "boto3.Session().resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'test/test_label.npy')).upload_file('test_label.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's clear the files we saved locally (they are already uploaded to s3 above)\n",
    "! rm train_data.npy train_label.npy validation_data.npy validation_label.npy test_data.npy test_label.npy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_location = 's3://{}/{}'.format(bucket, prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = MXNet(\"final_project.py\",\n",
    "          role=role,\n",
    "          train_instance_count=1,\n",
    "          train_instance_type=\"ml.p3.2xlarge\",\n",
    "          framework_version=\"1.2.1\",\n",
    "          py_version = 'py3',\n",
    "          hyperparameters={'batch_size': 50,\n",
    "                           'epochs': 50,\n",
    "                           'learning_rate': 0.02,\n",
    "                           'patience': 5\n",
    "                          })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-11-26 16:20:37 Starting - Starting the training job...\n",
      "2019-11-26 16:20:38 Starting - Launching requested ML instances...\n",
      "2019-11-26 16:21:37 Starting - Preparing the instances for training............\n",
      "2019-11-26 16:23:13 Downloading - Downloading input data......\n",
      "2019-11-26 16:24:38 Training - Training image download completed. Training in progress..\u001b[31m2019-11-26 16:24:38,906 INFO - root - running container entrypoint\u001b[0m\n",
      "\u001b[31m2019-11-26 16:24:38,906 INFO - root - starting train task\u001b[0m\n",
      "\u001b[31m2019-11-26 16:24:38,926 INFO - container_support.training - Training starting\u001b[0m\n",
      "\u001b[31m2019-11-26 16:24:40,123 WARNING - mxnet_container.train - #033[1;33mThis required structure for training scripts will be deprecated with the next major release of MXNet images. The train() function will no longer be required; instead the training script must be able to be run as a standalone script. For more information, see https://github.com/aws/sagemaker-python-sdk/tree/master/src/sagemaker/mxnet#updating-your-mxnet-training-script.#033[1;0m\u001b[0m\n",
      "\u001b[31m2019-11-26 16:24:43,164 INFO - mxnet_container.train - MXNetTrainingEnvironment: {'available_cpus': 4, 'job_name': 'sagemaker-mxnet-2019-11-26-16-20-36-216', '_scheduler_host': 'algo-1', 'output_dir': '/opt/ml/output', 'hyperparameters': {'sagemaker_job_name': 'sagemaker-mxnet-2019-11-26-16-20-36-216', 'epochs': 20, 'sagemaker_container_log_level': 20, 'patience': 5, 'batch_size': 32, 'sagemaker_enable_cloudwatch_metrics': False, 'sagemaker_region': 'eu-west-1', 'sagemaker_program': 'final_project.py', 'learning_rate': 0.01, 'sagemaker_submit_directory': 's3://sagemaker-eu-west-1-884265818273/sagemaker-mxnet-2019-11-26-16-20-36-216/source/sourcedir.tar.gz'}, 'channel_dirs': {'training': '/opt/ml/input/data/training'}, 'user_script_name': 'final_project.py', 'input_config_dir': '/opt/ml/input/config', 'container_log_level': 20, 'output_data_dir': '/opt/ml/output/data/', '_ps_verbose': 0, 'model_dir': '/opt/ml/model', 'channels': {'training': {'S3DistributionType': 'FullyReplicated', 'RecordWrapperType': 'None', 'TrainingInputMode': 'File'}}, 'base_dir': '/opt/ml', 'enable_cloudwatch_metrics': False, 'user_requirements_file': None, 'available_gpus': 1, 'sagemaker_region': 'eu-west-1', 'input_dir': '/opt/ml/input', 'current_host': 'algo-1', '_ps_port': 8000, 'hosts': ['algo-1'], 'resource_config': {'hosts': ['algo-1'], 'current_host': 'algo-1', 'network_interface_name': 'eth0'}, 'user_script_archive': 's3://sagemaker-eu-west-1-884265818273/sagemaker-mxnet-2019-11-26-16-20-36-216/source/sourcedir.tar.gz', 'code_dir': '/opt/ml/code', '_scheduler_ip': '10.0.119.117'}\u001b[0m\n",
      "\u001b[31mDownloading s3://sagemaker-eu-west-1-884265818273/sagemaker-mxnet-2019-11-26-16-20-36-216/source/sourcedir.tar.gz to /tmp/script.tar.gz\u001b[0m\n",
      "\u001b[31m2019-11-26 16:24:43,425 INFO - mxnet_container.train - Starting distributed training task\u001b[0m\n",
      "\u001b[31m/usr/local/lib/python3.5/dist-packages/mxnet_container/train.py:190: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() instead\n",
      "  train_args = inspect.getargspec(user_module.train)\u001b[0m\n",
      "\u001b[31m(1436, 3, 224, 224)\u001b[0m\n",
      "\u001b[31m(1436,)\u001b[0m\n",
      "\u001b[31m(180, 3, 224, 224)\u001b[0m\n",
      "\u001b[31m(180,)\u001b[0m\n",
      "\u001b[31m[16:25:04] src/operator/nn/./cudnn/./cudnn_algoreg-inl.h:107: Running performance tests to find the best convolution algorithm, this can take a while... (setting env variable MXNET_CUDNN_AUTOTUNE_DEFAULT to 0 to disable)\u001b[0m\n",
      "\u001b[31m[Epoch 0] Training: accuracy=0.385794 loss: 1.311527\u001b[0m\n",
      "\u001b[31m[Epoch 0] Validation: accuracy=0.411111 loss: 1.268969\u001b[0m\n",
      "\u001b[31m[Epoch 1] Training: accuracy=0.418524 loss: 1.254344\u001b[0m\n",
      "\u001b[31m[Epoch 1] Validation: accuracy=0.444444 loss: 1.233512\u001b[0m\n",
      "\u001b[31m[Epoch 2] Training: accuracy=0.431755 loss: 1.219130\u001b[0m\n",
      "\u001b[31m[Epoch 2] Validation: accuracy=0.466667 loss: 1.196487\u001b[0m\n",
      "\u001b[31m[Epoch 3] Training: accuracy=0.463788 loss: 1.178560\u001b[0m\n",
      "\u001b[31m[Epoch 3] Validation: accuracy=0.477778 loss: 1.162669\u001b[0m\n",
      "\u001b[31m[Epoch 4] Training: accuracy=0.490251 loss: 1.135440\u001b[0m\n",
      "\u001b[31m[Epoch 4] Validation: accuracy=0.522222 loss: 1.124123\u001b[0m\n",
      "\u001b[31m[Epoch 5] Training: accuracy=0.513231 loss: 1.091904\u001b[0m\n",
      "\u001b[31m[Epoch 5] Validation: accuracy=0.505556 loss: 1.097003\u001b[0m\n",
      "\u001b[31m[Epoch 6] Training: accuracy=0.532730 loss: 1.045643\u001b[0m\n",
      "\u001b[31m[Epoch 6] Validation: accuracy=0.572222 loss: 1.042285\u001b[0m\n",
      "\u001b[31m[Epoch 7] Training: accuracy=0.564763 loss: 0.996484\u001b[0m\n",
      "\u001b[31m[Epoch 7] Validation: accuracy=0.588889 loss: 0.991227\u001b[0m\n",
      "\u001b[31m[Epoch 8] Training: accuracy=0.600279 loss: 0.940168\u001b[0m\n",
      "\u001b[31m[Epoch 8] Validation: accuracy=0.627778 loss: 0.943660\u001b[0m\n",
      "\u001b[31m[Epoch 9] Training: accuracy=0.633705 loss: 0.881812\u001b[0m\n",
      "\u001b[31m[Epoch 9] Validation: accuracy=0.627778 loss: 0.903907\u001b[0m\n",
      "\u001b[31m[Epoch 10] Training: accuracy=0.674791 loss: 0.814325\u001b[0m\n",
      "\u001b[31m[Epoch 10] Validation: accuracy=0.650000 loss: 0.869691\u001b[0m\n",
      "\u001b[31m[Epoch 11] Training: accuracy=0.711699 loss: 0.738436\u001b[0m\n",
      "\u001b[31m[Epoch 11] Validation: accuracy=0.650000 loss: 0.842699\u001b[0m\n",
      "\u001b[31m[Epoch 12] Training: accuracy=0.743036 loss: 0.663319\u001b[0m\n",
      "\u001b[31m[Epoch 12] Validation: accuracy=0.666667 loss: 0.837473\u001b[0m\n",
      "\u001b[31m[Epoch 13] Training: accuracy=0.782730 loss: 0.596985\u001b[0m\n",
      "\u001b[31m[Epoch 13] Validation: accuracy=0.655556 loss: 0.835619\u001b[0m\n",
      "\u001b[31m[Epoch 14] Training: accuracy=0.824513 loss: 0.501076\u001b[0m\n",
      "\u001b[31m[Epoch 14] Validation: accuracy=0.661111 loss: 0.831261\u001b[0m\n",
      "\u001b[31m[Epoch 15] Training: accuracy=0.851671 loss: 0.430938\u001b[0m\n",
      "\u001b[31m[Epoch 15] Validation: accuracy=0.677778 loss: 0.866833\u001b[0m\n",
      "\u001b[31m[Epoch 16] Training: accuracy=0.878830 loss: 0.366080\u001b[0m\n",
      "\u001b[31m[Epoch 16] Validation: accuracy=0.655556 loss: 0.868155\u001b[0m\n",
      "\u001b[31m[Epoch 17] Training: accuracy=0.899025 loss: 0.317896\u001b[0m\n",
      "\u001b[31m[Epoch 17] Validation: accuracy=0.661111 loss: 0.878884\u001b[0m\n",
      "\u001b[31m[Epoch 18] Training: accuracy=0.950557 loss: 0.219571\u001b[0m\n",
      "\u001b[31m[Epoch 18] Validation: accuracy=0.672222 loss: 0.851463\u001b[0m\n",
      "\u001b[31m[Epoch 19] Training: accuracy=0.951950 loss: 0.180410\u001b[0m\n",
      "\u001b[31m[Epoch 19] Validation: accuracy=0.666667 loss: 0.912662\u001b[0m\n",
      "\n",
      "2019-11-26 16:26:28 Uploading - Uploading generated training model\n",
      "2019-11-26 16:26:43 Completed - Training job completed\n",
      "Training seconds: 210\n",
      "Billable seconds: 210\n"
     ]
    }
   ],
   "source": [
    "m.fit(data_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------------!"
     ]
    }
   ],
   "source": [
    "predictor = m.deploy(instance_type='ml.m4.xlarge', initial_instance_count=1, endpoint_name='mla-cv-endpoint')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's test this endpoint\n",
    "We will read the test data from our S3 buckets and call the endpoint we just created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from io import BytesIO\n",
    "import mxnet.ndarray as nd\n",
    "import boto3\n",
    "\n",
    "sm = boto3.client('s3')\n",
    "obj = sm.get_object(Bucket='gmenende-mla-training', Key='mla-cv-sagemaker-demo/test/test_data.npy')\n",
    "X_test = np.load(BytesIO(obj['Body'].read()))\n",
    "\n",
    "obj = sm.get_object(Bucket='gmenende-mla-training', Key='mla-cv-sagemaker-demo/test/test_label.npy')\n",
    "y_test = np.load(BytesIO(obj['Body'].read()))\n",
    "\n",
    "payload = X_test[:4]\n",
    "payload = bytearray(payload)\n",
    "\n",
    "runtime = boto3.Session().client(service_name='runtime.sagemaker')\n",
    "response = runtime.invoke_endpoint(EndpointName=\"mla-cv-endpoint\", \n",
    "                                   ContentType='application/x-image', \n",
    "                                   Body=payload)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'[2.0, 2.0, 4.0, 4.0]'\n"
     ]
    }
   ],
   "source": [
    "print(response['Body'].read())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_amazonei_mxnet_p36",
   "language": "python",
   "name": "conda_amazonei_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
