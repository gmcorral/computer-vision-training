{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![MLA Logo](https://drive.corp.amazon.com/view/mrruckma@/MLA_headerv2.png?download=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In this example, we will do an advanced task: Adding new layers to a pre-trained network. We will add another fully connected layer (along with its dropout) and the last softmax layer to a pre-trained Alexnet. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In this section, we will first create \"our own\" Alexnet. Then, we will copy the weights from a pre-trained alexnet by matching the names of the layers. Be careful with the naming of the layers in the pre-trained network and ours. We will match layers by their names, their shapes should also match.\n",
    "#### Alexnet has the following architecture, let's start building it below.\n",
    "![alexnet](https://drive.corp.amazon.com/view/cesazara@/cv-notebook-images/alexnet.png?download=true)\n",
    "\n",
    "#### We will implement a custom Alexnet where we add antoher Fully Connected Layer with 4096 neurons. Our custom network will be like this:\n",
    "![custom alexnet](https://drive.corp.amazon.com/view/cesazara@/cv-notebook-images/custom_alexnet.png?download=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mxnet as mx\n",
    "from mxnet import gluon\n",
    "from mxnet.gluon import nn\n",
    "from mxnet.gluon.model_zoo import vision\n",
    "import mxnet.ndarray as nd\n",
    "\n",
    "# Set this to GPU or CPU\n",
    "# ctx = mx.gpu()\n",
    "ctx = mx.cpu()\n",
    "\n",
    "# Define custom model, we will add an extra fully connected layer with dropout differently from original.\n",
    "alex_net = gluon.nn.Sequential()\n",
    "alex_net.add(nn.Conv2D(64, kernel_size=11, strides=4, padding=2, activation='relu'))\n",
    "alex_net.add(nn.MaxPool2D(pool_size=3, strides=2))\n",
    "alex_net.add(nn.Conv2D(192, kernel_size=5, padding=2, activation='relu'))\n",
    "alex_net.add(nn.MaxPool2D(pool_size=3, strides=2))\n",
    "alex_net.add(nn.Conv2D(384, kernel_size=3, padding=1, activation='relu'))\n",
    "alex_net.add(nn.Conv2D(256, kernel_size=3, padding=1, activation='relu'))\n",
    "alex_net.add(nn.Conv2D(256, kernel_size=3, padding=1, activation='relu'))\n",
    "alex_net.add(nn.MaxPool2D(pool_size=3, strides=2))\n",
    "alex_net.add(nn.Flatten())\n",
    "alex_net.add(nn.Dense(4096, activation='relu'))\n",
    "alex_net.add(nn.Dropout(0.5))\n",
    "alex_net.add(nn.Dense(4096, activation='relu'))\n",
    "alex_net.add(nn.Dropout(0.5))\n",
    "\n",
    "# must initialize parameters before changing,\n",
    "# so pass through example batch since lazy initialization\n",
    "alex_net.initialize(ctx=ctx)\n",
    "alex_net(nd.random.uniform(shape=(10, 3, 224, 224)).as_in_context(ctx))\n",
    "\n",
    "# load pretrained model\n",
    "pretrained_alex_net = vision.alexnet(pretrained=True, ctx=ctx)\n",
    "\n",
    "# create parameter dictionaries\n",
    "model_params = {name: param for name, param in alex_net.collect_params().items()}\n",
    "pretrained_model_params = {name: param for name, param in pretrained_alex_net.collect_params().items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's print our custom model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2D(3 -> 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2), Activation(relu))\n",
       "  (1): MaxPool2D(size=(3, 3), stride=(2, 2), padding=(0, 0), ceil_mode=False, global_pool=False, pool_type=max, layout=NCHW)\n",
       "  (2): Conv2D(64 -> 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), Activation(relu))\n",
       "  (3): MaxPool2D(size=(3, 3), stride=(2, 2), padding=(0, 0), ceil_mode=False, global_pool=False, pool_type=max, layout=NCHW)\n",
       "  (4): Conv2D(192 -> 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), Activation(relu))\n",
       "  (5): Conv2D(384 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), Activation(relu))\n",
       "  (6): Conv2D(256 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), Activation(relu))\n",
       "  (7): MaxPool2D(size=(3, 3), stride=(2, 2), padding=(0, 0), ceil_mode=False, global_pool=False, pool_type=max, layout=NCHW)\n",
       "  (8): Flatten\n",
       "  (9): Dense(9216 -> 4096, Activation(relu))\n",
       "  (10): Dropout(p = 0.5, axes=())\n",
       "  (11): Dense(4096 -> 4096, Activation(relu))\n",
       "  (12): Dropout(p = 0.5, axes=())\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alex_net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's print the pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AlexNet(\n",
       "  (features): HybridSequential(\n",
       "    (0): Conv2D(3 -> 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2), Activation(relu))\n",
       "    (1): MaxPool2D(size=(3, 3), stride=(2, 2), padding=(0, 0), ceil_mode=False, global_pool=False, pool_type=max, layout=NCHW)\n",
       "    (2): Conv2D(64 -> 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), Activation(relu))\n",
       "    (3): MaxPool2D(size=(3, 3), stride=(2, 2), padding=(0, 0), ceil_mode=False, global_pool=False, pool_type=max, layout=NCHW)\n",
       "    (4): Conv2D(192 -> 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), Activation(relu))\n",
       "    (5): Conv2D(384 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), Activation(relu))\n",
       "    (6): Conv2D(256 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), Activation(relu))\n",
       "    (7): MaxPool2D(size=(3, 3), stride=(2, 2), padding=(0, 0), ceil_mode=False, global_pool=False, pool_type=max, layout=NCHW)\n",
       "    (8): Flatten\n",
       "    (9): Dense(9216 -> 4096, Activation(relu))\n",
       "    (10): Dropout(p = 0.5, axes=())\n",
       "    (11): Dense(4096 -> 4096, Activation(relu))\n",
       "    (12): Dropout(p = 0.5, axes=())\n",
       "  )\n",
       "  (output): Dense(4096 -> 1000, linear)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_alex_net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We will match the layers by their names. Let's print them below.\n",
    "#### Pay attention to the naming difference between pre-trained and custom model. We have an extra \"alexnet0_\" substring in the parameter names of the pre-trained network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-trained Alexnet Parameters\n",
      "{'alexnet0_conv0_weight': Parameter alexnet0_conv0_weight (shape=(64, 3, 11, 11), dtype=<class 'numpy.float32'>), 'alexnet0_conv0_bias': Parameter alexnet0_conv0_bias (shape=(64,), dtype=<class 'numpy.float32'>), 'alexnet0_conv1_weight': Parameter alexnet0_conv1_weight (shape=(192, 64, 5, 5), dtype=<class 'numpy.float32'>), 'alexnet0_conv1_bias': Parameter alexnet0_conv1_bias (shape=(192,), dtype=<class 'numpy.float32'>), 'alexnet0_conv2_weight': Parameter alexnet0_conv2_weight (shape=(384, 192, 3, 3), dtype=<class 'numpy.float32'>), 'alexnet0_conv2_bias': Parameter alexnet0_conv2_bias (shape=(384,), dtype=<class 'numpy.float32'>), 'alexnet0_conv3_weight': Parameter alexnet0_conv3_weight (shape=(256, 384, 3, 3), dtype=<class 'numpy.float32'>), 'alexnet0_conv3_bias': Parameter alexnet0_conv3_bias (shape=(256,), dtype=<class 'numpy.float32'>), 'alexnet0_conv4_weight': Parameter alexnet0_conv4_weight (shape=(256, 256, 3, 3), dtype=<class 'numpy.float32'>), 'alexnet0_conv4_bias': Parameter alexnet0_conv4_bias (shape=(256,), dtype=<class 'numpy.float32'>), 'alexnet0_dense0_weight': Parameter alexnet0_dense0_weight (shape=(4096, 9216), dtype=float32), 'alexnet0_dense0_bias': Parameter alexnet0_dense0_bias (shape=(4096,), dtype=float32), 'alexnet0_dense1_weight': Parameter alexnet0_dense1_weight (shape=(4096, 4096), dtype=float32), 'alexnet0_dense1_bias': Parameter alexnet0_dense1_bias (shape=(4096,), dtype=float32), 'alexnet0_dense2_weight': Parameter alexnet0_dense2_weight (shape=(1000, 4096), dtype=float32), 'alexnet0_dense2_bias': Parameter alexnet0_dense2_bias (shape=(1000,), dtype=float32)}\n",
      "Custom Alexnet Parameters\n",
      "{'conv0_weight': Parameter conv0_weight (shape=(64, 3, 11, 11), dtype=<class 'numpy.float32'>), 'conv0_bias': Parameter conv0_bias (shape=(64,), dtype=<class 'numpy.float32'>), 'conv1_weight': Parameter conv1_weight (shape=(192, 64, 5, 5), dtype=<class 'numpy.float32'>), 'conv1_bias': Parameter conv1_bias (shape=(192,), dtype=<class 'numpy.float32'>), 'conv2_weight': Parameter conv2_weight (shape=(384, 192, 3, 3), dtype=<class 'numpy.float32'>), 'conv2_bias': Parameter conv2_bias (shape=(384,), dtype=<class 'numpy.float32'>), 'conv3_weight': Parameter conv3_weight (shape=(256, 384, 3, 3), dtype=<class 'numpy.float32'>), 'conv3_bias': Parameter conv3_bias (shape=(256,), dtype=<class 'numpy.float32'>), 'conv4_weight': Parameter conv4_weight (shape=(256, 256, 3, 3), dtype=<class 'numpy.float32'>), 'conv4_bias': Parameter conv4_bias (shape=(256,), dtype=<class 'numpy.float32'>), 'dense0_weight': Parameter dense0_weight (shape=(4096, 9216), dtype=float32), 'dense0_bias': Parameter dense0_bias (shape=(4096,), dtype=float32), 'dense1_weight': Parameter dense1_weight (shape=(4096, 4096), dtype=float32), 'dense1_bias': Parameter dense1_bias (shape=(4096,), dtype=float32)}\n"
     ]
    }
   ],
   "source": [
    "print(\"Pre-trained Alexnet Parameters\")\n",
    "print(pretrained_model_params)\n",
    "print(\"Custom Alexnet Parameters\")\n",
    "print(model_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's transfer weights by matching the names of the parameters below. We will have mismatches for the new added layers and "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sucessful match for conv0_weight.\n",
      "Sucessful match for conv0_bias.\n",
      "Sucessful match for conv1_weight.\n",
      "Sucessful match for conv1_bias.\n",
      "Sucessful match for conv2_weight.\n",
      "Sucessful match for conv2_bias.\n",
      "Sucessful match for conv3_weight.\n",
      "Sucessful match for conv3_bias.\n",
      "Sucessful match for conv4_weight.\n",
      "Sucessful match for conv4_bias.\n",
      "Sucessful match for dense0_weight.\n",
      "Sucessful match for dense0_bias.\n",
      "Sucessful match for dense1_weight.\n",
      "Sucessful match for dense1_bias.\n"
     ]
    }
   ],
   "source": [
    "for name, param in model_params.items():\n",
    "    lookup_name = 'alexnet0_' + name\n",
    "    if lookup_name in pretrained_model_params:\n",
    "        lookup_param = pretrained_model_params[lookup_name]\n",
    "        if lookup_param.shape == param.shape:\n",
    "            param.set_data(lookup_param.data())\n",
    "            print(\"Sucessful match for {}.\".format(name))\n",
    "        else:\n",
    "            print(\"Error: Shape mismatch for {}. {}!={}\".format(name, lookup_param.shape, param.shape))\n",
    "    else:\n",
    "        print(\"Error: Couldn't find match for {}.\".format(name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In this part, we will add the remainder of the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "alex_net.add(nn.Dense(4096, activation='relu'))\n",
    "alex_net.add(nn.Dropout(0.5))\n",
    "alex_net.add(nn.Dense(4096, activation='relu'))\n",
    "alex_net.add(nn.Dropout(0.5))\n",
    "alex_net.add(nn.Dense(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2D(3 -> 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2), Activation(relu))\n",
       "  (1): MaxPool2D(size=(3, 3), stride=(2, 2), padding=(0, 0), ceil_mode=False, global_pool=False, pool_type=max, layout=NCHW)\n",
       "  (2): Conv2D(64 -> 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), Activation(relu))\n",
       "  (3): MaxPool2D(size=(3, 3), stride=(2, 2), padding=(0, 0), ceil_mode=False, global_pool=False, pool_type=max, layout=NCHW)\n",
       "  (4): Conv2D(192 -> 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), Activation(relu))\n",
       "  (5): Conv2D(384 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), Activation(relu))\n",
       "  (6): Conv2D(256 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), Activation(relu))\n",
       "  (7): MaxPool2D(size=(3, 3), stride=(2, 2), padding=(0, 0), ceil_mode=False, global_pool=False, pool_type=max, layout=NCHW)\n",
       "  (8): Flatten\n",
       "  (9): Dense(9216 -> 4096, Activation(relu))\n",
       "  (10): Dropout(p = 0.5, axes=())\n",
       "  (11): Dense(4096 -> 4096, Activation(relu))\n",
       "  (12): Dropout(p = 0.5, axes=())\n",
       "  (13): Dense(None -> 4096, Activation(relu))\n",
       "  (14): Dropout(p = 0.5, axes=())\n",
       "  (15): Dense(None -> 4096, Activation(relu))\n",
       "  (16): Dropout(p = 0.5, axes=())\n",
       "  (17): Dense(None -> 5, linear)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alex_net"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_mxnet_p36",
   "language": "python",
   "name": "conda_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
